{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex11 - augmentation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPuCn3JX/9+bC7+LpwtGjzy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Young1906/shopee_product_detection/blob/main/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "cvObn9wWpjIR",
        "outputId": "08b1e5aa-5eaa-4434-9332-b46be2462b37"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "from tqdm import tqdm \r\n",
        "import random, time, os\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# from google.colab import drive\r\n",
        "# drive.mount(\"/gdrive\")\r\n",
        "\r\n",
        "!test -d data && ls data || unzip /gdrive/MyDrive/dataset/shopee-code-league-2020-product-detection.zip -d data 1>/dev/null\r\n",
        "\r\n",
        "# Tutorial\r\n",
        "# https://www.kaggle.com/fadheladlansyah/product-detection-effnetb5-aug-tta\r\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resized  test.csv  train.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKYoMIalMuav"
      },
      "source": [
        "## 0. Utilities function\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WX1McJsNZaJ"
      },
      "source": [
        "## 1. Input pipeline\r\n",
        "- Dataset + preprocessing\r\n",
        "- Augmentation\r\n",
        "\r\n",
        "### 1.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyIhIuwrNp6b"
      },
      "source": [
        "# Config\r\n",
        "AUTO = tf.data.experimental.AUTOTUNE\r\n",
        "BATCH_SIZE = 32\r\n",
        "PTH = \"data/\"\r\n",
        "CACHE_PTH = \".\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtO9l7mhWy6P"
      },
      "source": [
        "# Constructing dataset\r\n",
        "def f(val_size = .1):\r\n",
        "    assert val_size < 1, ValueError(\"Validation size < 1.\")\r\n",
        "\r\n",
        "    # Padding function\r\n",
        "    pad = lambda x: f\"0{x}\" if x < 10 else f\"{x}\"\r\n",
        "    \r\n",
        "    # train.csv, test.csv\r\n",
        "    train = pd.read_csv(f\"{PTH}/train.csv\")\r\n",
        "    test = pd.read_csv(f\"{PTH}/test.csv\")\r\n",
        "\r\n",
        "    # Constructing file path\r\n",
        "    train['fn_pth'] = train.apply(lambda x: f\"{PTH}/resized/train/{pad(x['category'])}/{x['filename']}\", axis = 1)\r\n",
        "    test['fn_pth'] = test.apply(lambda x: f\"{PTH}/resized/test/{x['filename']}\", axis = 1)\r\n",
        "\r\n",
        "    train = train[[\"fn_pth\", \"category\"]]\r\n",
        "    test = test[[\"fn_pth\", \"category\"]]\r\n",
        "\r\n",
        "    # Shuffle train data\r\n",
        "    n_train, _ = train.shape\r\n",
        "    train = train.iloc[np.random.permutation(n_train),:]\r\n",
        "\r\n",
        "    X_train, y_train = train[\"fn_pth\"].values, train[\"category\"].values\r\n",
        "    X_test, y_test = test[\"fn_pth\"].values, test[\"category\"].values\r\n",
        "\r\n",
        "    # Validation split\r\n",
        "    X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = val_size)\r\n",
        "\r\n",
        "    # clean up shit\r\n",
        "    del train, test, n_train, pad\r\n",
        "\r\n",
        "    # TF's Dataset API\r\n",
        "    train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\n",
        "    validation = tf.data.Dataset.from_tensor_slices((X_validate, y_validate))\r\n",
        "    test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\r\n",
        "    \r\n",
        "    return train, validation, test\r\n",
        "\r\n",
        "def make_dataset(dataset):\r\n",
        "    global AUTO, BATCH_SIZE, CACHE_PTH\r\n",
        "\r\n",
        "    def load_img(pth, label):\r\n",
        "        def f(pth, label):\r\n",
        "            # read the img\r\n",
        "            img = np.asarray(Image.open(pth.numpy()), dtype = np.uint8)\r\n",
        "            img = tf.constant(img)\r\n",
        "            # convert label to uint 8\r\n",
        "            label = np.uint8(label)\r\n",
        "            return img, label\r\n",
        "        # Convert f to TF's function\r\n",
        "        return tf.py_function(f, (pth, label), (tf.uint8, tf.uint8))\r\n",
        "\r\n",
        "    dataset = dataset \\\r\n",
        "        .map(load_img, num_parallel_calls=AUTO) \\\r\n",
        "        .batch(BATCH_SIZE) \\\r\n",
        "        .cache(CACHE_PTH) \\\r\n",
        "        .shuffle(2048)\r\n",
        "\r\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWUXsuC1dbIB"
      },
      "source": [
        "train, valid, test = f()\r\n",
        "train = make_dataset(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_mwhKfSSnC9"
      },
      "source": [
        "# Define network\r\n",
        "class Preprocess(tf.keras.layers.Layer):\r\n",
        "    def __init__(self):\r\n",
        "        super(Preprocess, self).__init__()\r\n",
        "    \r\n",
        "    def call(self, X):\r\n",
        "        X = tf.keras.applications.efficientnet.preprocess_input(X)\r\n",
        "        return X\r\n",
        "\r\n",
        "def make_model():\r\n",
        "    with tf.device(\"/device:GPU:0\"):\r\n",
        "        basemodel = tf.keras.applications.EfficientNetB7(\r\n",
        "            include_top=\"False\",\r\n",
        "            weights=\"imagenet\"\r\n",
        "        )\r\n",
        "        basemodel.trainable = False\r\n",
        "\r\n",
        "        net = tf.keras.Sequential([\r\n",
        "            Preprocess(),\r\n",
        "            basemodel,\r\n",
        "            tf.keras.layers.Dropout(.2),\r\n",
        "            tf.keras.layers.Dense(42, activation = \"softmax\")\r\n",
        "        ])\r\n",
        "\r\n",
        "        net.compile(optimizer = \"sgd\",\r\n",
        "                    loss=\"sparse_categorical_crossentropy\",\r\n",
        "                    metrics=[\"accuracy\"]\r\n",
        "                    )\r\n",
        "        net.build(input_shape=(299,299,3))\r\n",
        "        net.summary() \r\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoaZp7EM8SFb"
      },
      "source": [
        "for i in train:\r\n",
        "    print(i);break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}